# Causal inference to analyze the effect of medicines on COVID-19
## Introduction
- The COVID-19 pandemic boosted attempts to identify and develop effective drug treatments. In this project, we applied causal inference methods to analyze the impact of medications on COVID-19 patient outcomes(find out whether the medication actually helps the patient recover or not).

## Causal Inference
- Normally, finding out whether the treatments (medicines in this case) actually have a good effect on the patient or not is a challenging task due to the confounders (the variables that can affect the treatments and the outcome, such as age, gender, etc.) <br>
![images](https://github.com/user-attachments/assets/b1d3682a-c12a-43e2-9d03-2d68eaae3eda) <br>
- The confounders can create the treatment bias, which is when the doctor tends to give a certain treatment to the patients who have a certain confounder (doctors like to give treatment A to people who are less than 30 years old and treatment B to people who are more than 65 years old).
This creates the imbalance and unfairness in the data. Also, the confounder can affect the outcome (young people will recover faster and have a higher chance of survival compared to old people. This makes treatment A appear to be more effective than treatment B.)
- In order to address these problems, a Randomized Controlled Trial (RCT) is often performed. IN RCT, the confounders are controlled during the experiment to ensure fair and accurate results. Even though RCT is the most ideal and accurate method to measure the effectiveness of treatments, it is not always possible to perform and can be expensive to perform.
- In order to find the effectiveness of a treatment when RCT is not possible, we can use other Causal Inference techniques.  
- Causal inference techniques allow us to estimate the effect of treatments using observational data. This observational data is the dataset that contains the information about the patients (such as their medical conditions(heart disease, fever, etc.,), ages, gender, etc.,), that treatment (medicines) did the patient take and the outcome of after taking that treatment(whether they are alive (recovered) or not).
We will use 2 causal inference techniques: Backdoor Adjustment and propensity Score matching (PSM).

## Dataset
- The dataset is a subset from the National Covid Cohort Collaborative (N3C) with 10,000 instances. It contains patients’ information during the COVID-19 pandemic, making it a great resource for researching the possible treatment effects of drugs. The dataset included:
  - Patient demographics: include age (represented as continuous variables), gender, race, ethnicity (encoded), and geographic location (zip code). These demographic variables are crucial characteristics that could affect the process of treatment as well as the results of the illness.
  - Medical conditions are represented in a “conditions” column containing lists of numerical condition codes that are SNOMED IDs. These codes include important details on patient characteristics that may influence treatment choices and results, such as heart disease, fever, etc. Each patient will have a list of SNOMED IDs in the condition column.
  - Treatment variables: consist of 16 kinds of different drugs (trazodone, amitriptyline, fluoxetine, citalopram, paroxetine, venlafaxine, vilazodone, vortioxetine, sertraline, bupropion, mirtazapine, desvenlafaxine, doxepin, duloxetine, escitalopram, nortriptyline)
  - Outcome: provides a binary value for patient survival or recovery (1) or non-survival or death (0), in addition to an additional variable that indicates the severity of COVID-related deaths. The evaluation of treatment efficiency in improving patient survival is made possible by these outcome indicators.
 
### Data Preprocessing:
In order to prepare the dataset for causal inference analysis, we converted the dataset into a suitable format. We preprocess data based on the following steps:
1.	Encoding the condition: Since each patient has a list of SNOMED-ID in the condition columns, there could be thousands of different SNOMED-ID in the dataset, and doing one-hot encoding to separate them may not be effective and will be computationally expensive later. To solve this issue, we used Word2Vec to convert each SNOMED-ID into a vector embedding of size 5. This gives us a list of vector embeddings for each patient. Then, we compute the pooled embedding that summarizes the conditions that a patient has by calculating the average of the vector embeddings in the list of conditions of that patient. This effectively gives us a single pooled embedding for each patient that represents all the medical conditions that they have.  
2.	We also removed 2 redundant columns named severity_covid_death and zip.
3.	After obtaining the pooled embedding for each patient, since each pooled embedding vector has 5 values, we split them into 5 different columns named condition1, condition2, condition3, condition4, condition5 so that we can implement the algorithms for causal inference easier. 
4.	To further preprocess the dataset for Backdoor Adjustment, we remove the 5 condition columns. Also, we need to convert the continuous variable to categorical because the Backdoor Adjustment does not work with continuous variables. For the 5 condition columns and age, we turn the upper half to 1 and the lower half to 0. For the columns with more than 2 categories, we convert half of each category to 0 and the other half to 1. 
5.	To further preprocess the dataset for PSM, we do not do step 4 but create a copy of the dataset and do other preprocessing. We first numerically encode the categorical variables. Then, because our dataset has a mix of continuous and categorical features with different distributions and values, which is not good for training models like logistic regression, we decided to min-max scale the variables (except for the 5 conditions columns since they are already in a small range of values) 



## Algorithm details and Implementations
### Backdoor Adjustment
- Backdoor adjustment allows us to estimate the causal effect of a treatment by controlling for the confounders. In observational data, the confounders like age, gender, etc., can both influence the treatments that a patient would take and the outcome (whether the patient recovers or not). Controlling the confounders allows us to account for their effect on the treatments and outcomes and leads to more accurate results by removing the non-causal association between the treatments and the outcomes.

- In order to compute the average treatment effect (ATE) for a treatment using Backdoor Adjustment, we calculate the causal effect considering each confounder at a time. <br>
![image](https://github.com/user-attachments/assets/b5f7282a-ce77-435c-86a9-b012f8856965) <br>
- In the formula above. We iterate over each value (category) of the current confounder. `N(T, X=i)` is the number of patients who have taken treatment T and have the value i for this confounder. `N(X=i)` is the number of patients who have value i for the current confounder. `P(Y = 1 | T, X= i)` is the probability that a patient survives given they take treatment T and have value i for this confounder. This essentially gives us the treatment effect of the treatment, considering one confounder.
- Since we have multiple confounders, we need to do the same thing for each of them and get multiple treatment effects. Then, we find the average of these treatments to get the average treatment effect (ATE) of a treatment.
- Backdoor Adjustment is the most straightforward algorithm and is easy to implement. However, this algorithm only works for categorical values. If the variables are continuous, then we need to use a discretization technique, such as using logistic regression, to turn the continuous values into categorical values. 
### Propensity Score Matching (PSM) to compute ATE
- PSM is also a technique to estimate the causal effects of treatments in observational data by accounting for confounders. But unlike Backdoor Adjustment, PSM simulates a randomized Controlled Trial (RCT) by matching each patient in the treatment group with another patient in the control group. Calculating. Calculating ATE using PSM consists of multiple steps:
  1.	Calculating the propensity score for the patients in the treatment group and control group. The propensity score is the probability that a patient takes the treatment given the covariate. Propensity scores are used to group or match patients with similar covariates, even if they didn’t receive the same treatment. This can be computed by training a logistic regression model using the data of people who took the treatment (treatment group) and the control group, and whether that patient took the treatment as labels.
  2.	Matching each patient who took the treatment with another patient who did not take the treatment but has a similar propensity score (closest to the propensity score of the patient who took the treatment). This is done using KNN to find 1 nearest neighbor. Matching allows us to get the counterfactual, which is the outcome of the patient who takes the treatment if we did not give them the treatment. The outcome of the matched patient in the control group will be used as the counterfactual.
  3.	Calculating ATE. The ATE can be calculated by finding the average of the outcome of people who took the treatment (the sum for the average is simply just adding all the 0s and 1s even though the outcome is a categorical feature) and subtract by the average of the outcome of people who did not take the treatment. The result of this subtraction is the ATE. The ATE will also be calculated by using logistic regression. First, we train a logistic regression using the treatment and matched control group as training data, with whether they recover (outcome 1) or not as labels. Then, we use the weight for the treatment variables in the logistic regression as ATE. If the ATE is positive, then the treatment is helpful, but if the result is negative, then the treatment can be harmful.

### Propensity Score Matching (PSM) to compute CATE
- PSM seems to be a more efficient technique than Backdoor adjustment. However, it has a major weakness when calculating the propensity scores using logistic regression. Due to the imbalance in the dataset, there are many more people who did not take the treatment compared to the number of people who took the treatment. This imbalance makes it really hard for the logistic model to learn to predict correctly for the minority class. To address this issue, we use bootstrapping. We repeatedly choose a subset of datapoints from the control group randomly so that it would have the same number of datapoints as in the treatment group, then we compute the propensity score, do matching, and calculate ATE like before. Repeat this entire process 500 times, and we will obtain a list of ATE for the treatment. Next, we calculate the confidence interval at 95%. With the mean as our best estimate for the CATE (cumulative average treatment effect). We will also obtain the lower bound and upper bound for the 95% confidence interval. This information essentially says “we are 95% confident that the true treatment effect is within the lower bound and upper bound range, with the best guess of what the treatment effect is the mean.”

## Results
The results of the project are detailed in the `Project Report.pdf` file in the `Results` folder. 

